{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, write_gt_csv\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.geometry import transform_point, transform_points\n",
    "from torch import Tensor\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import l5kit\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision.models.resnet import resnet18, resnet50, resnet34\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nikhil: changed history step size to 3 and num frames to 5\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'history_num_frames': 5,\n",
    "        'history_step_size': 3,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [300, 300],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "        'disable_traffic_light_faces': False\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 2\n",
    "    },\n",
    "    \n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 64,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'val_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maerial_map\u001b[0m/                       \u001b[01;34mscenes\u001b[0m/\r\n",
      "meta.json                         \u001b[01;34msemantic_map\u001b[0m/\r\n",
      "multi_mode_sample_submission.csv  single_mode_sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls /root/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = \"/root/input\"\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "train_zarr = ChunkedDataset(dm.require(cfg['train_data_loader'][\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             shuffle=cfg['train_data_loader'][\"shuffle\"], #nikhil: should be shuffled, train set\n",
    "                             batch_size=cfg['train_data_loader'][\"batch_size\"],\n",
    "                             num_workers=cfg['train_data_loader'][\"num_workers\"],\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zarr = ChunkedDataset(dm.require(cfg['val_data_loader'][\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                             shuffle=cfg['val_data_loader'][\"shuffle\"],\n",
    "                             batch_size=cfg['val_data_loader'][\"batch_size\"],\n",
    "                             num_workers=cfg['val_data_loader'][\"num_workers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nikhil: changed to resnet18\n",
    "        self.backbone = resnet18(pretrained=False)\n",
    "        \n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "        \n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "        # You can add more layers here.\n",
    "        # nikhil: adding some layers here\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (\n",
    "        batch_size,\n",
    "        future_len,\n",
    "        num_coords,\n",
    "    ), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (\n",
    "        batch_size,\n",
    "        num_modes,\n",
    "    ), f\"expected 1D (Modes) array for confidences, got {confidences.shape}\"\n",
    "    assert torch.allclose(\n",
    "        torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))\n",
    "    ), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (\n",
    "        batch_size,\n",
    "        future_len,\n",
    "    ), f\"expected 1D (Time) array for avails, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(\n",
    "        ((gt - pred) * avails) ** 2, dim=-1\n",
    "    )  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(\n",
    "        divide=\"ignore\"\n",
    "    ):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(\n",
    "        dim=1, keepdim=True\n",
    "    )  # error are negative at this point, so max() gives the minimum one\n",
    "    error = (\n",
    "        -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True))\n",
    "        - max_value\n",
    "    )  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# compiling model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = LyftModel(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "criterion_mse = nn.MSELoss(reduction=\"none\")\n",
    "criterion = pytorch_neg_multi_log_likelihood_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transform_points(points: torch.Tensor, transf_matrix: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        points (torch.Tensor): Input points (BxNx2) or (BxNx3).\n",
    "        transf_matrix (torch.Tensor): Bx3x3 or Bx4x4 transformation matrix for 2D and 3D input respectively\n",
    "    Returns:\n",
    "        torch.Tensor: array of shape (B,N,2) for 2D input points, or (B,N,3) points for 3D input points\n",
    "    \"\"\"\n",
    "    assert len(points.shape) == len(transf_matrix.shape) == 3\n",
    "    assert transf_matrix.shape[1] == transf_matrix.shape[2]\n",
    "\n",
    "    if points.shape[2] not in [2, 3]:\n",
    "        raise AssertionError(\"Points input should be (B, N, 2) or (B, N,3) shape, received {}\".format(points.shape))\n",
    "\n",
    "    assert points.shape[2] == transf_matrix.shape[2] - 1, \"points dim should be one less than matrix dim\"\n",
    "\n",
    "    num_dims = len(transf_matrix[0]) - 1\n",
    "\n",
    "    transf_matrix = transf_matrix.transpose(1, 2)\n",
    "    # print(points.shape, transf_matrix.shape)\n",
    "\n",
    "    # print(torch.bmm(points, transf_matrix[:, :num_dims, :num_dims]).shape)\n",
    "    # print(transf_matrix[:, -1:-1, :num_dims].shape)\n",
    "    return torch.bmm(points, transf_matrix[:, :num_dims, :num_dims]) + transf_matrix[:, -1, :num_dims].reshape(len(transf_matrix), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = torch.load(\"models/resnet_18_0.5690epoch_steps_vast_ai_oct_16_2_2020.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/351512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 73.48468017578125 loss(avg): 86.85670573936834:   0%|          | 349/351512 [11:01<196:27:34,  2.01s/it] "
     ]
    }
   ],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "val_it = iter(val_dataloader)\n",
    "\n",
    "# val_data = next(val_it)\n",
    "\n",
    "progress_bar = tqdm(range(len(train_dataloader)))\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "print('starting')\n",
    "\n",
    "# plt.fig()\n",
    "\n",
    "for iter_ind in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # forward pass\n",
    "    inputs = data[\"image\"].to(device)\n",
    "#     target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)    #MSE\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "#     targets = batch_transform_points(data[\"target_positions\"].float().to(device), data[\"raster_from_agent\"].float().to(device))\n",
    "    \n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(targets, outputs.reshape((outputs.shape[0], 1, outputs.shape[1], outputs.shape[2])), Tensor(np.ones((outputs.shape[0], 1))).to(device), target_availabilities)\n",
    "#     loss_mse = criterion_mse(outputs, targets)    #MSE\n",
    "    \n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "#     loss_mse = loss_mse * target_availabilities.unsqueeze(-1)    #MSE\n",
    "#     loss_mse = loss_mse.mean()\n",
    "#     print(loss_mse)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    plot_freq = 1000\n",
    "    \n",
    "    if iter_ind % plot_freq == 0 and iter_ind != 0:\n",
    "#         plt.plot(outputs)\n",
    "        with torch.no_grad():\n",
    "            plt.plot(range(iter_ind), losses_train)\n",
    "            model.eval()\n",
    "            val_loss_list = []\n",
    "            for val_indx in range(200):\n",
    "                try:\n",
    "                    val_data = next(val_it)\n",
    "                except StopIteration:\n",
    "                    val_it = iter(val_dataloader)\n",
    "                    val_data = next(val_it)\n",
    "                val_inputs = val_data[\"image\"].to(device)\n",
    "#                 val_target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device) # MSE\n",
    "                val_target_availabilities = val_data[\"target_availabilities\"].to(device)\n",
    "                val_targets = val_data[\"target_positions\"].to(device)\n",
    "#                 val_targets = batch_transform_points(val_data[\"target_positions\"].float().to(device), val_data[\"raster_from_agent\"].float().to(device))\n",
    "                val_outputs = model(val_inputs).reshape(val_targets.shape)\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "\n",
    "                # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "#                 val_loss = val_loss * val_target_availabilities # MSE\n",
    "#                 val_loss = val_loss.mean()   # MSE\n",
    "                val_loss_list.append(val_loss)\n",
    "\n",
    "        \n",
    "        vlm = sum(val_loss_list)/len(val_loss_list)\n",
    "        losses_val.append(vlm.cpu().detach().numpy())\n",
    "        print('val_loss:', vlm)\n",
    "        plt.plot(range(plot_freq,iter_ind+1, plot_freq), losses_val, color=\"red\")\n",
    "        torch.save(model.state_dict(), 'models/resnet_18_{:4.4f}epoch_steps_vast_ai_oct_24_2020.pth'.format(100*iter_ind/len(train_dataloader)))\n",
    "        plt.savefig(\"models/resnet_18_{:4.4f}epoch_steps_vast_ai_oct_24_2020.jpg\".format(100*iter_ind/len(train_dataloader)))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'resnet_18_1epoch_steps_vast_ai_oct_11_2020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "progress_bar = tqdm(range(20000))\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "print('starting')\n",
    "\n",
    "# plt.fig()\n",
    "\n",
    "for iter_ind in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # forward pass\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = batch_transform_points(data[\"target_positions\"].float().to(device), data[\"raster_from_agent\"].float().to(device))\n",
    "    \n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    plot_freq = 1000\n",
    "    \n",
    "    if iter_ind % plot_freq == 0 and iter_ind != 0:\n",
    "        with torch.no_grad():\n",
    "            plt.plot(range(iter_ind), losses_train)\n",
    "            model.eval()\n",
    "            val_loss_list = []\n",
    "            val_len = 1000\n",
    "            for val_indx in range(val_len):\n",
    "                try:\n",
    "                    val_data = next(val_it)\n",
    "                except StopIteration:\n",
    "                    val_it = iter(val_dataloader)\n",
    "                    val_data = next(val_it)\n",
    "                val_inputs = val_data[\"image\"].to(device)\n",
    "                val_target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "                val_targets = batch_transform_points(val_data[\"target_positions\"].float().to(device), val_data[\"raster_from_agent\"].float().to(device))\n",
    "                val_outputs = model(val_inputs).reshape(val_targets.shape)\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "\n",
    "                # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "                val_loss = val_loss * val_target_availabilities\n",
    "                val_loss = val_loss.mean()\n",
    "                val_loss_list.append(val_loss)\n",
    "\n",
    "        \n",
    "        vlm = sum(val_loss_list)/len(val_loss_list)\n",
    "        losses_val.append(vlm.cpu().detach().numpy())\n",
    "        print('val_loss:', vlm)\n",
    "        plt.scatter(range(plot_freq,iter_ind+1, plot_freq), losses_val, color=\"red\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'resnet_18_30k_oct_8th.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "\n",
    "agent_ids = []\n",
    "\n",
    "losses_val = []\n",
    "progress_bar = tqdm(validate_dataloader)\n",
    "for idx, data in enumerate(progress_bar):\n",
    "    \n",
    "#     if idx == 5:\n",
    "#         break\n",
    "    \n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    \n",
    "    loss = criterion(outputs, targets)\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    losses_val.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_val)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
