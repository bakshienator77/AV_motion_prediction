{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, write_gt_csv\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.geometry import transform_point, transform_points\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import l5kit\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision.models.resnet import resnet18, resnet50, resnet34\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nikhil: changed history step size to 3 and num frames to 5\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'history_num_frames': 5,\n",
    "        'history_step_size': 3,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [150, 150],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 2\n",
    "    },\n",
    "    \n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2\n",
    "    },\n",
    "    \n",
    "    'val_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 64,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 2\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maerial_map\u001b[0m/                       \u001b[01;34mscenes\u001b[0m/\r\n",
      "meta.json                         \u001b[01;34msemantic_map\u001b[0m/\r\n",
      "multi_mode_sample_submission.csv  single_mode_sample_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls /root/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = \"/root/input\"\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "train_zarr = ChunkedDataset(dm.require(cfg['train_data_loader'][\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             shuffle=False,\n",
    "                             batch_size=8,\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zarr = ChunkedDataset(dm.require(cfg['val_data_loader'][\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                             shuffle=False,\n",
    "                             batch_size=8,\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nikhil: changed to resnet18\n",
    "        self.backbone = resnet18(pretrained=True)\n",
    "        \n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        \n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "        \n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "        # You can add more layers here.\n",
    "        # nikhil: adding some layers here\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# compiling model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = LyftModel(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transform_points(points: torch.Tensor, transf_matrix: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        points (torch.Tensor): Input points (BxNx2) or (BxNx3).\n",
    "        transf_matrix (torch.Tensor): Bx3x3 or Bx4x4 transformation matrix for 2D and 3D input respectively\n",
    "    Returns:\n",
    "        torch.Tensor: array of shape (B,N,2) for 2D input points, or (B,N,3) points for 3D input points\n",
    "    \"\"\"\n",
    "    assert len(points.shape) == len(transf_matrix.shape) == 3\n",
    "    assert transf_matrix.shape[1] == transf_matrix.shape[2]\n",
    "\n",
    "    if points.shape[2] not in [2, 3]:\n",
    "        raise AssertionError(\"Points input should be (B, N, 2) or (B, N,3) shape, received {}\".format(points.shape))\n",
    "\n",
    "    assert points.shape[2] == transf_matrix.shape[2] - 1, \"points dim should be one less than matrix dim\"\n",
    "\n",
    "    num_dims = len(transf_matrix[0]) - 1\n",
    "\n",
    "    transf_matrix = transf_matrix.transpose(1, 2)\n",
    "    # print(points.shape, transf_matrix.shape)\n",
    "\n",
    "    # print(torch.bmm(points, transf_matrix[:, :num_dims, :num_dims]).shape)\n",
    "    # print(transf_matrix[:, -1:-1, :num_dims].shape)\n",
    "    return torch.bmm(points, transf_matrix[:, :num_dims, :num_dims]) + transf_matrix[:, -1, :num_dims].reshape(len(transf_matrix), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sane\n"
     ]
    }
   ],
   "source": [
    "print('sane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "val_it = iter(val_dataloader)\n",
    "\n",
    "# val_data = next(val_it)\n",
    "\n",
    "progress_bar = tqdm(range(10000))\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "print('starting')\n",
    "\n",
    "# plt.fig()\n",
    "\n",
    "for iter_ind in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # forward pass\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = batch_transform_points(data[\"target_positions\"].float().to(device), data[\"raster_from_agent\"].float().to(device))\n",
    "    \n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter_ind % 100 == 0 and iter_ind != 0:\n",
    "        with torch.no_grad():\n",
    "            plt.plot(range(iter_ind), losses_train)\n",
    "            model.eval()\n",
    "            val_loss_list = []\n",
    "            for val_indx in range(100):\n",
    "                try:\n",
    "                    val_data = next(val_it)\n",
    "                except StopIteration:\n",
    "                    val_it = iter(val_dataloader)\n",
    "                    val_data = next(val_it)\n",
    "                val_inputs = val_data[\"image\"].to(device)\n",
    "                val_target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "                val_targets = batch_transform_points(val_data[\"target_positions\"].float().to(device), val_data[\"raster_from_agent\"].float().to(device))\n",
    "                val_outputs = model(val_inputs).reshape(val_targets.shape)\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "\n",
    "                # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "                val_loss = val_loss * val_target_availabilities\n",
    "                val_loss = val_loss.mean()\n",
    "                val_loss_list.append(val_loss)\n",
    "\n",
    "        \n",
    "        vlm = sum(val_loss_list)/len(val_loss_list)\n",
    "        losses_val.append(vlm.cpu().detach().numpy())\n",
    "        print('val_loss:', vlm)\n",
    "        plt.scatter(range(100,iter_ind+1, 100), losses_val, color=\"red\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(758.2779, dtype=float32), array(1138.66, dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f82d1cc0390>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(100,iter_ind+1, 100), losses_val, color=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'resnet_18_10k_steps_vast_ai_oct_7_2020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(10000))\n",
    "\n",
    "losses_train = []\n",
    "\n",
    "print('starting')\n",
    "\n",
    "for iter_ind in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # forward pass\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = batch_transform_points(data[\"target_positions\"].float().to(device), data[\"raster_from_agent\"].float().to(device))\n",
    "    \n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter_ind % 100 == 0 and iter_ind != 0:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        val_target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "        val_targets = batch_transform_points(val_data[\"target_positions\"].float().to(device), val_data[\"raster_from_agent\"].float().to(device))\n",
    "\n",
    "        val_outputs = model(val_inputs).reshape(val_targets.shape)\n",
    "        val_loss = criterion(val_outputs, val_targets)\n",
    "        \n",
    "        # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "        val_loss = val_loss * val_target_availabilities\n",
    "        val_loss = val_loss.mean()\n",
    "        \n",
    "        print('val_loss:', val_loss)\n",
    "        \n",
    "        \n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'resnet_34_30k_oct_5th.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "\n",
    "agent_ids = []\n",
    "\n",
    "losses_val = []\n",
    "progress_bar = tqdm(validate_dataloader)\n",
    "for idx, data in enumerate(progress_bar):\n",
    "    \n",
    "#     if idx == 5:\n",
    "#         break\n",
    "    \n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    \n",
    "    loss = criterion(outputs, targets)\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    losses_val.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_val)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
